{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ram.models import ram\n",
    "from ram import get_transform\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parsing Data Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_path(feature_dir='./distilled_keyframe'):\n",
    "    all_feature_paths = dict()\n",
    "    for feature_part in sorted(os.listdir(feature_dir)):\n",
    "        all_feature_paths[feature_part] = dict()\n",
    "    for feature_part in sorted(all_feature_paths.keys()):\n",
    "        feature_part_path = f'{feature_dir}/{feature_part}'\n",
    "        feature_paths = sorted(os.listdir(feature_part_path))\n",
    "        feature_ids = [feature_path.split('.')[0] for feature_path in feature_paths]\n",
    "        for feature_id, feature_path in zip(feature_ids, feature_paths):\n",
    "            feature_path_full = f'{feature_part_path}/{feature_path}'\n",
    "            all_feature_paths[feature_part][feature_id] = feature_path_full\n",
    "    return all_feature_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:/AIC2024/extra/recognize-anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downdload Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_checkpoints(model):\n",
    "    print('You selected', model)\n",
    "    if not os.path.exists('pretrained'):\n",
    "        os.makedirs('pretrained')\n",
    "\n",
    "    if model == \"RAM\":\n",
    "        ram_weights_path = 'pretrained/ram_swin_large_14m.pth'\n",
    "        if not os.path.exists(ram_weights_path):\n",
    "            !wget https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/ram_swin_large_14m.pth -O pretrained/ram_swin_large_14m.pth\n",
    "        else:\n",
    "            print(\"RAM weights already downloaded!\")\n",
    "\n",
    "model = \"RAM\"\n",
    "download_checkpoints(model)\n",
    "print(model, 'weights are downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Function Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "@torch.no_grad()\n",
    "def forward_ram(model, imgs):\n",
    "    image_embeds = model.image_proj(model.visual_encoder(imgs.to(device)))\n",
    "    image_atts = torch.ones(\n",
    "        image_embeds.size()[:-1], dtype=torch.long).to(device)\n",
    "    label_embed = torch.nn.functional.relu(model.wordvec_proj(model.label_embed)).unsqueeze(0)\\\n",
    "        .repeat(imgs.shape[0], 1, 1)\n",
    "    tagging_embed, _ = model.tagging_head(\n",
    "        encoder_embeds=label_embed,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        encoder_attention_mask=image_atts,\n",
    "        return_dict=False,\n",
    "        mode='tagging',\n",
    "    )\n",
    "    bs = imgs.shape[0]\n",
    "    logits = torch.sigmoid(model.fc(tagging_embed).squeeze(-1))\n",
    "    targets = torch.where(\n",
    "        logits > model.class_threshold.to(device),\n",
    "        torch.tensor(1.0).to(device),\n",
    "        torch.zeros(model.num_class).to(device))\n",
    "\n",
    "    tag = targets.cpu().numpy()\n",
    "    tag_outputs = []\n",
    "    tag_logits = []\n",
    "    for b in range(bs):\n",
    "        index = np.argwhere(tag[b] == 1)\n",
    "        tokens = model.tag_list[index].squeeze(axis=1)\n",
    "        scores = logits[b][index[:, 0]]\n",
    "        tag_outputs.append([token.replace(\" \", \"_\") for token in tokens])\n",
    "        tag_logits.append(scores.cpu().numpy())\n",
    "\n",
    "    return tag_outputs, tag_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform(image_size=384)\n",
    "model = ram(pretrained='pretrained/ram_swin_large_14m.pth',\n",
    "            image_size=384,\n",
    "            vit='swin_l')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "tag_list = model.tag_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_paths, transform):\n",
    "    images = [transform(Image.open(image_path)).unsqueeze(0) for image_path in image_paths]\n",
    "    return images\n",
    "\n",
    "def encode_tags(model, images):\n",
    "    tag_outputs, tag_logits = forward_ram(model, images)\n",
    "    tag_contexts = []\n",
    "\n",
    "    for index in range(len(tag_outputs)):\n",
    "        tag_context = []\n",
    "        tag_output, tag_logit = tag_outputs[index], tag_logits[index]\n",
    "        tag_frequency = np.round(tag_logit * 10).astype(int)\n",
    "\n",
    "        for tag, freq in zip(tag_output, tag_frequency):\n",
    "            tag_context.extend([tag] * freq)\n",
    "\n",
    "        tag_context = ' '.join(map(str, tag_context))\n",
    "        tag_contexts.append(tag_context)\n",
    "\n",
    "    return tag_contexts\n",
    "\n",
    "def write_json_file(json_data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:/AIC2024/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframe_dir='./distilled_keyframe'\n",
    "all_keyframe_paths = parse_data_path(feature_dir=keyframe_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_id(keyframe_paths):\n",
    "    id_path_keyframes = []\n",
    "    for keyframe_path in keyframe_paths:\n",
    "        keyframe_filename = keyframe_path.split('/')[-1]\n",
    "        keyframe_id = int(keyframe_filename.split('.')[0])\n",
    "        id_path_keyframes.append((keyframe_id, keyframe_path))\n",
    "    sorted_id_path_keyframes = sorted(id_path_keyframes, key=lambda id_path: id_path[0])\n",
    "    return [id_path[1] for id_path in sorted_id_path_keyframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "save_dir = './filter/tag/features'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for video_part, video_path_dict in all_keyframe_paths.items():\n",
    "    video_ids = video_path_dict.keys()\n",
    "    full_save_dir = os.path.join(save_dir, video_part)\n",
    "    os.makedirs(full_save_dir, exist_ok=True)\n",
    "\n",
    "    for video_id in tqdm(video_ids, desc=f'Encoding Part {video_part}'):\n",
    "        video_id_metadata_records = {}\n",
    "        video_id_path = video_path_dict[video_id]\n",
    "        keyframe_image_paths = [os.path.join(video_id_path, keyframe_image_path) for keyframe_image_path in os.listdir(video_id_path)]\n",
    "        keyframe_image_paths = sorted_by_id(keyframe_image_paths)\n",
    "        for i in range(0, len(keyframe_image_paths), batch_size):\n",
    "            image_paths = keyframe_image_paths[i:i+batch_size]\n",
    "            images = load_images(image_paths, transform)\n",
    "            images = torch.cat(images).to(device)\n",
    "            tag_contexts = encode_tags(model, images)\n",
    "            for image_path, tag_context in zip(image_paths, tag_contexts):\n",
    "                video_id_metadata_records[image_path] = {\n",
    "                    'tag': tag_context\n",
    "                }\n",
    "        write_json_file(video_id_metadata_records, os.path.join(full_save_dir, f'{video_id}.json'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
