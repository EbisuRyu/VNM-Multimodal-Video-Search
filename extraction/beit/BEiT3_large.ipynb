{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from extra.unilm.beit3.utils import load_model_and_may_interpolate\n",
    "from extra.unilm.beit3.modeling_finetune import beit3_large_patch16_384_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:/AIC2024/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parsing Data Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_path(feature_dir='./keyframe'):\n",
    "    all_feature_paths = dict()\n",
    "    for feature_part in sorted(os.listdir(feature_dir)):\n",
    "        all_feature_paths[feature_part] = dict()\n",
    "    for feature_part in sorted(all_feature_paths.keys()):\n",
    "        feature_part_path = f'{feature_dir}/{feature_part}'\n",
    "        feature_paths = sorted(os.listdir(feature_part_path))\n",
    "        feature_ids = [feature_path.split('.')[0] for feature_path in feature_paths]\n",
    "        for feature_id, feature_path in zip(feature_ids, feature_paths):\n",
    "            feature_path_full = f'{feature_part_path}/{feature_path}'\n",
    "            all_feature_paths[feature_part][feature_id] = feature_path_full\n",
    "    return all_feature_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_paths = parse_data_path(feature_dir='./distilled_keyframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BEiT3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:/AIC2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_path = './dict/beit/weights/beit3_large_itc_patch16_224.pth'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "beit3_model = beit3_large_patch16_384_retrieval(pretrained=False)\n",
    "load_model_and_may_interpolate(model_weight_path, beit3_model, model_key='model', model_prefix='')\n",
    "beit3_model.to(device)\n",
    "beit3_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_paths, batch_size, image_size=384):\n",
    "    id2image_fps = {}\n",
    "    video_features, images = [], []\n",
    "    for id, image_path in enumerate(image_paths):\n",
    "        id2image_fps[id] = image_path\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size),\n",
    "                            interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        raw_image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(raw_image).unsqueeze(0).to(device)\n",
    "        images.append(image)\n",
    "\n",
    "    images = torch.cat(images, dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for start_index in range(0, images.shape[0], batch_size):\n",
    "            image_features, _ = beit3_model(image=images[start_index:start_index+batch_size], only_infer=True)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            for index in range(image_features.shape[0]):\n",
    "                video_features.append(image_features[index].cpu().numpy().astype(np.float32).flatten())\n",
    "    return id2image_fps, video_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_id(keyframe_paths):\n",
    "    id_path_keyframes = []\n",
    "    for keyframe_path in keyframe_paths:\n",
    "        keyframe_filename = keyframe_path.split('/')[-1]\n",
    "        keyframe_id = int(keyframe_filename.split('.')[0])\n",
    "        id_path_keyframes.append((keyframe_id, keyframe_path))\n",
    "    sorted_id_path_keyframes = sorted(id_path_keyframes, key=lambda id_path: id_path[0])\n",
    "    return [id_path[1] for id_path in sorted_id_path_keyframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2image_save_dir='./beit/large/id2image'\n",
    "feature_save_dir=\"./beit/large/features\"\n",
    "if not os.path.exists(id2image_save_dir):\n",
    "    os.makedirs(id2image_save_dir)\n",
    "if not os.path.exists(feature_save_dir):\n",
    "    os.makedirs(feature_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for video_part, video_path_dict in all_video_paths.items():\n",
    "    video_ids = video_path_dict.keys()\n",
    "    for video_id in tqdm(video_ids, desc=f'Encoding Part {video_part}'):\n",
    "        video_id_path = video_path_dict[video_id]\n",
    "        keyframe_image_paths = [video_id_path + '/' + keyframe_image_path for keyframe_image_path in os.listdir(video_id_path)]\n",
    "        sorted_keyframe_image_paths = sorted_by_id(keyframe_image_paths)\n",
    "        id2image_fps, video_features = encode_images(sorted_keyframe_image_paths, batch_size)\n",
    "\n",
    "        os.makedirs(f'{feature_save_dir}/{video_part}', exist_ok=True)\n",
    "        np.save(f'{feature_save_dir}/{video_part}/{video_id}.npy', video_features)\n",
    "\n",
    "        os.makedirs(f'{id2image_save_dir}/{video_part}', exist_ok=True)\n",
    "        with open(f'{id2image_save_dir}/{video_part}/{video_id}.json', 'w') as f:\n",
    "            json.dump(id2image_fps, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
